{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概念介绍\n",
    "\n",
    "token: 最小语义单元\n",
    "\n",
    "tokenizer: 分词器，每个模型都会有自己的tokenizer，将输入文本划分为token\n",
    "\n",
    "tokenization: 分词的过程\n",
    "\n",
    "流程\n",
    "\n",
    "输入文本， token， 模型推理， 输出   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline\n",
    "\n",
    "简化上述全部中间过程\n",
    "\n",
    "transformer中会预设一些类型的模型（如下），当你选择一个类型时，它会去下载一个该类型的模型（具体下载哪个不清楚），然后使用该模型。\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# # # 实体命名识别\n",
    "# ner = pipeline(\"ner\", grouped_entities=True)\n",
    "# print(ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\"))\n",
    "\n",
    "# # # 掩码填充\n",
    "# fill_mask = pipeline(\"fill-mask\")\n",
    "# print(fill_mask(\"The cat is <mask> on the mat.\"))\n",
    "\n",
    "# # # 翻译\n",
    "# translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "# print(translator(\"Ce cours est produit par Hugging Face.\"))\n",
    "\n",
    "# # # 文章总结\n",
    "# summarizer = pipeline(\"summarization\")\n",
    "# print(summarizer(\"xxxxxxxxxxxxxxxxxx\"))\n",
    "\n",
    "# # # 文本生成\n",
    "# generator = pipeline(\"text-generation\")\n",
    "# print(generator(\"In this course, we will teach you how to\"))\n",
    "# # # 指定hugging face Hub网站中任意模型\n",
    "# generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "# print(generator(\"In this course, we will teach you how to\", max_length=30, num_return_sequences=2))\n",
    "\n",
    "# # # 问题回答\n",
    "# question_answerer = pipeline(\"question-answering\")\n",
    "# print(question_answerer(question=\"Where do I work?\", context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"))\n",
    "\n",
    "# # 情绪分析\n",
    "classifier = pipeline(\"sentiment-analysis\")   # # （该库只能输入英文。）\n",
    "print(classifier([\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]))\n",
    "\n",
    "# # # 零样本分类zero-shot-classification\n",
    "# classifier = pipeline(\"zero-shot-classification\")\n",
    "# print(classifier(\"This is a course about the Transformers library\", candidate_labels=[\"education\", \"politics\", \"business\"]))\n",
    "\n",
    "# # # 获取文本的向量表示\n",
    "# feature_extraction = pipeline(\"feature-extraction\") \n",
    "# print(feature_extraction(\"i am a studet\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998829364776611}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 情绪分析\n",
    "pipe = pipeline(\"sentiment-analysis\")   # （该模型只能处理英文。）\n",
    "print(pipe([\"This course is amazing.\", \"I hate this so much!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面使用的是 sentiment-analysis, 它下载了 distilbert/distilbert-base-uncased-finetuned-sst-2-english 这个模型\n",
    "另外也可以使用pipe下载指定的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998829364776611}, {'label': 'NEGATIVE', 'score': 0.9994558691978455}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "print(pipe([\"This course is amazing.\", \"I hate this so much!\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常规流程\n",
    "### 转为token\n",
    "\n",
    "将输入的原始文本raw text 转换为tokens（单词words、子单词subwords、符号symbols 都算）\n",
    "将每个tokens 与一个整数做映射，得到Input IDs；\n",
    "同时也会添加可能对模型有用的额外输入，比如起始字符、隔断字符等；\n",
    "同一个单词在不同模型的tokenizer下得到的Input IDs数值可能不同，而Input IDs数值必须要与当前使用的预训练模型保持通用。为此，AutoModel 和AutoTokenizer 请共用一个模型检查点checkpoint\n",
    "\n",
    "padding：填充，确保所有样本在序列长度上是一致的，如果样本的长度小于既定长度，将用填充字符进行补齐；\n",
    "truncation：截断，确保所有样本序列长度不超过既定长度，若超过，进行截断处理；\n",
    "max_length ：既定长度，默认为True，不同模型的既定长度可能不一样，也可以用数值指定；\n",
    "return_tensors：指定返回的张量类型，“pt”：PyTorch，“tf”：TensorFlow ，“np”：NumPy；\n",
    "\n",
    "不同标志器编码输出的张量结构也存在一定差异，这个是包含两个键的字典：\n",
    "\n",
    "input_ids：每一行代表一个编码后的句子，0 表示填充字符\n",
    "attention_mask：1 表示要注意该位标记，0 表示不注意该位标记（即忽略该位），主要用于self-attention层\n",
    "\n",
    "input_ids：代表编码后的输出IDs；\n",
    "token_type_ids：是在BERT模型中用于表示不同句子的标识符，标识哪些部分是前句子，哪些部分是后句子；\n",
    "attention_mask：主要用于self-attention层。1 表示要注意相应位置的标记，0 表示忽略相应位置的标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2607, 2003, 6429, 1012,  102,    0],\n",
      "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# # 加载标志器，模型\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_text = [\"This course is amazing.\", \"I hate this so much!\"]\n",
    "\n",
    "tokens = tokenizer(raw_text, padding=True, truncation=True, max_length = 10, return_tensors=\"pt\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['this', 'course', 'is', 'amazing', '.', 'i', 'hate', 'this', 'so', 'much', '!']\n",
      "{'input_ids': tensor([[ 101, 2023, 2607, 2003, 6429, 1012,  102,    0],\n",
      "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# # 加载标志器，模型\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_text = [\"This course is amazing.\", \"I hate this so much!\"]\n",
    "\n",
    "\n",
    "tokenized_sequence  = tokenizer.tokenize(raw_text)\n",
    "tokens = tokenizer(raw_text, padding=True, truncation=True, max_length = 10, return_tensors=\"pt\")\n",
    "\n",
    "print(tokenized_sequence)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型预测\n",
    "和标志器tokenizer 一样，Transformers 也提供了一个AutoModel 类和from_pretrained() 方法——在使用该模型检查点checkpoint 时，自动下载、加载、使用该模型。\n",
    "而在具体任务中，模型需要根据不同的任务需求，选择加载不同的模型头model head。模型头是附加组件，通常由一层或几层组成，用于将预测转换为特定任务的输出，下面是一个非详尽的模型头列表：（下表里的 -xxx 表示省略，请与前文保存一致）\n",
    "\n",
    "*ForCausalLM：因果语言模型，用于文本生成、对话生成，例如GPT2ForCausalLM，XLNetForCausalLM；\n",
    "*ForMaskedLM：掩码模型，用于理解语言的上下文和语境，例如BERTForMaskedLM、RobertaForMaskedLM；\n",
    "*ForMultipleChoice：多项选择模型，常见于问答系统，例如BertForMultipleChoice、XLNetForMultipleChoice；\n",
    "*ForQuestionAnswering：问答模型，例如BertFor-xxx、DistilBertFor-xxx、XLNetForQ-xxx；\n",
    "*ForSequenceClassification：序列分类模型，常见于文本分类、情感分析，例如BertFor-xxx、DistilBertFor-xxx、XLNetFor-xxx；\n",
    "*ForTokenClassification：标记分类模型，常见于命名实体识别，例如BertFor-xxx、DistilBertFor-xxx、XLNetFor-xxx；\n",
    "*Model：用于检索模型架构的隐藏状态\n",
    "其他\n",
    "对于我们的示例，我们需要模型具有序列分类的能力（能够将句子分类为正例或负例），因此我们实际不会使用AutoModel 用于初始化，而是会根据具体需求使用AutoModelForSequenceClassification 这个模型头；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2607, 2003, 6429, 1012,  102,    0],\n",
      "        [ 101, 1045, 5223, 2023, 2061, 2172,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "tensor([[-4.3690,  4.6833],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# # 加载标志器，模型\n",
    "checkpoint = \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_text = [\"This course is amazing.\", \"I hate this so much!\"]\n",
    "\n",
    "tokens = tokenizer(raw_text, padding=True, truncation=True, max_length = 10, return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**tokens)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 后处理\n",
    "我们从模型中获得的输出值本身并不一定有意义，例如上例中的输出，它并不是概率，而是logits，要转换为概率，还需要经过SoftMax层，这样就可以看到输出的概率；\n",
    "再配合每个位置对应的标签：\n",
    "第一句：有0.0117%的概率是NEGATIVE，只有99.988%的概率是POSITIVE\n",
    "第二句：有99.946%的概率是NEGATIVE，有0.0544%的概率是POSITIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1711e-04, 9.9988e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)\n",
    "\n",
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存模型\n",
    "还是以BERT 模型为例，使用配置参数BertConfig 创建模型，以随机数权重进行初始化；\n",
    "可以输出BERT 模型的配置参数，这些参数包含许多用于构建模型的基本属性信息；\n",
    "\n",
    "上述创建模型的方式是使用随机权重进行的初始化，相当于创建了一个全新的、未训练的模型。该状态下的模型也可以使用，但会输出乱码，需要注入数据 + 需要时间训练——不必重复造轮子，请使用共享的、已经训练好的预训练模型吧！\n",
    "\n",
    "保存模型\n",
    "保存模型就像加载模型一样简单，保存模型用：\n",
    "model.save_pretrained(\"model_path\")\n",
    "\n",
    "将会有两类文件保存在model_path目录下，这两类文件缺一不可；\n",
    "配置文件：保存模型架构，权重文件：保存模型权重（权重文件可能有多个）。\n",
    "\n",
    "config.json          # # 配置文件：保存构建模型架构所需的所有属性\n",
    "pytorch_model.bin    # # 权重文件：保存模型所有的权重数值，可能有多个.bin文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# # 读取配置参数\n",
    "config = BertConfig()\n",
    "print(config)\n",
    "# # 配置参数构建模型\n",
    "model = BertModel(config)   # # 模型是随机初始化的！\n",
    "\n",
    "# BertConfig {\n",
    "#   [...]\n",
    "#   \"hidden_size\": 768,              # # 隐藏层中神经元的数量\n",
    "#   \"intermediate_size\": 3072,       # # FNN前馈网络中间层的维度\n",
    "#   \"max_position_embeddings\": 512,  # # 最大序列长度，也是位置矩阵的长度\n",
    "#   \"num_attention_heads\": 12,       # # 注意力头的数量\n",
    "#   \"num_hidden_layers\": 12,         # # 隐藏层的数量\n",
    "#   [...]\n",
    "# }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
